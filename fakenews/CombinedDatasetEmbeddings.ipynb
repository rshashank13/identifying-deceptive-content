{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                                              title  label\n",
      "109859      551743  jose fernandez’s final words to pregnant girlf...      0\n",
      "234403      429276             when the senate gives you an option bc      0\n",
      "228787      121307  did fidel castro nearly have a career in profe...      1\n",
      "147185      185929  algal biofuel at competitive prices being eval...      0\n",
      "202050      857621  this dream job pays $$10k monthly, requires yo...      0\n",
      "...            ...                                                ...    ...\n",
      "200518      165258  small children dead in pennsylvania house fire...      1\n",
      "24185        79135  you can live control the black robot on the ri...      1\n",
      "200587      374599           north koreas overton window c to present      0\n",
      "208033      417509                  utah gets first porndetecting dog      1\n",
      "22879       282034                            httpsiimgurcomrmtabujpg      0\n",
      "\n",
      "[480000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv(\"fakeddit-train.csv\")\n",
    "b = pd.read_csv(\"fake-real-train.csv\")\n",
    "c = pd.concat([a,b])\n",
    "c  = c.sample(frac=1)\n",
    "print(c)\n",
    "traindf = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                                              title  label\n",
      "7189       395839  4min news september 13, 2013: cnn, nasa vids, ...      0\n",
      "22451       62440  continental coal welcomes black economic empow...      0\n",
      "19028      344345                                build your own site      0\n",
      "24343       25070  not winning any popularity contests with this one      0\n",
      "25828      416188                                 mexico's third way      1\n",
      "...           ...                                                ...    ...\n",
      "12917       13286  epa chief signs proposal limiting science used...      1\n",
      "2584         2640  this bar in ireland has a rocket launcher hang...      1\n",
      "10335       29170  evangelical alliance: bell's 'love wins' void ...      1\n",
      "20649       21251                                          say aahhh      0\n",
      "29328      210305     giving hope to jonathan after violent shooting      1\n",
      "\n",
      "[117455 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv(\"fakeddit-test.csv\")\n",
    "b = pd.read_csv(\"fake-real-test.csv\")\n",
    "c = pd.concat([a,b])\n",
    "c  = c.sample(frac=1)\n",
    "print(c)\n",
    "testdf = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                                              title  label\n",
      "109859      551743  jose fernandez’s final words to pregnant girlf...      0\n",
      "234403      429276             when the senate gives you an option bc      0\n",
      "228787      121307  did fidel castro nearly have a career in profe...      1\n",
      "147185      185929  algal biofuel at competitive prices being eval...      0\n",
      "202050      857621  this dream job pays $$10k monthly, requires yo...      0\n",
      "...            ...                                                ...    ...\n",
      "200518      165258  small children dead in pennsylvania house fire...      1\n",
      "24185        79135  you can live control the black robot on the ri...      1\n",
      "200587      374599           north koreas overton window c to present      0\n",
      "208033      417509                  utah gets first porndetecting dog      1\n",
      "22879       282034                            httpsiimgurcomrmtabujpg      0\n",
      "\n",
      "[480000 rows x 3 columns]\n",
      "       Unnamed: 0                                              title  label\n",
      "7189       395839  4min news september 13, 2013: cnn, nasa vids, ...      0\n",
      "22451       62440  continental coal welcomes black economic empow...      0\n",
      "19028      344345                                build your own site      0\n",
      "24343       25070  not winning any popularity contests with this one      0\n",
      "25828      416188                                 mexico's third way      1\n",
      "...           ...                                                ...    ...\n",
      "12917       13286  epa chief signs proposal limiting science used...      1\n",
      "2584         2640  this bar in ireland has a rocket launcher hang...      1\n",
      "10335       29170  evangelical alliance: bell's 'love wins' void ...      1\n",
      "20649       21251                                          say aahhh      0\n",
      "29328      210305     giving hope to jonathan after violent shooting      1\n",
      "\n",
      "[117455 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "traindf['title'] = traindf['title'].apply(str).apply(str.lower)\n",
    "print(traindf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "testdf['title'] = testdf['title'].apply(str).apply(str.lower)\n",
    "print(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 768])\n",
      "torch.Size([768])\n",
      "tensor([-1.5751e-02,  1.7555e-02, -9.9957e-03, -9.2712e-02, -6.0308e-03,\n",
      "        -9.9498e-02, -3.5565e-02,  3.9437e-03,  1.1180e-01, -5.5667e-02,\n",
      "        -1.7163e-02,  7.2778e-02,  3.0900e-02, -1.9077e-02,  6.8676e-02,\n",
      "         3.4866e-02, -6.6551e-02,  1.2368e-02, -2.5138e-02, -3.2812e-02,\n",
      "        -1.1089e-01,  6.0594e-02, -1.6723e-02,  1.1750e-01,  2.1872e-02,\n",
      "         9.8197e-03,  8.4527e-02,  6.7308e-02, -7.3586e-02, -6.1845e-02,\n",
      "        -6.4132e-02,  5.3634e-03,  2.3297e-02, -1.6054e-02,  5.6944e-02,\n",
      "         9.3123e-02,  7.9736e-02, -3.5234e-02, -1.2442e-01,  3.0586e-02,\n",
      "        -1.9112e-02, -2.7735e-02,  2.7157e-02,  6.7165e-03,  6.2950e-02,\n",
      "         3.6358e-02,  3.9580e-03,  2.5430e-02, -5.1409e-02,  2.9367e-03,\n",
      "         1.3888e-02,  9.0640e-02, -5.6213e-02,  4.8685e-02, -9.1531e-02,\n",
      "         9.0456e-03, -1.9976e-02,  9.6132e-02,  5.9358e-02, -9.0505e-03,\n",
      "         6.6105e-02, -9.4679e-02, -9.5924e-02, -6.9349e-03,  3.4017e-02,\n",
      "        -8.1517e-03, -2.9317e-02, -3.3893e-02,  4.7303e-02,  1.5923e-02,\n",
      "         8.2824e-02, -3.9254e-02,  7.0135e-02, -2.0907e-02, -4.0746e-02,\n",
      "        -1.2763e-02,  4.3458e-02,  4.5407e-01, -4.4640e-02,  9.1189e-03,\n",
      "         5.6888e-02, -3.3691e-02,  3.0234e-01,  7.0582e-03,  2.8533e-02,\n",
      "        -1.5218e-02,  3.9297e-02,  4.9695e-02,  4.2950e-02,  1.1932e-02,\n",
      "         1.2564e-02,  6.7515e-02, -9.1203e-02,  4.9388e-02,  5.7958e-03,\n",
      "         3.1207e-02,  3.3729e-02, -4.8246e-02, -3.4313e-02, -5.4224e-02,\n",
      "         8.4308e-03, -6.0164e-02,  6.6051e-02,  5.8659e-02,  7.0337e-03,\n",
      "         1.4864e-02,  4.3660e-02,  1.3920e-02,  8.7634e-03, -4.6633e-02,\n",
      "        -2.1272e-02,  4.4728e-02,  1.2029e-02,  3.8241e-02, -2.7868e-02,\n",
      "        -4.5977e-02,  2.3247e-02, -1.4237e-03,  1.3726e-02,  3.4572e-02,\n",
      "         3.5859e-02,  9.5682e-02,  7.9048e-02, -4.5518e-03, -6.3810e-03,\n",
      "        -2.3040e-02,  8.9388e-03,  4.0757e-02, -2.2796e-02,  6.1161e-02,\n",
      "        -2.5154e-02, -7.2168e-02, -6.6698e-04,  1.0870e-01,  7.5724e-02,\n",
      "         2.5946e-02,  7.0423e-02,  1.8220e-02,  8.4206e-02, -2.6181e-03,\n",
      "        -7.3347e-03,  6.1279e-02,  9.1046e-02,  5.9541e-02,  9.0286e-02,\n",
      "         1.6157e-02,  1.6507e-02, -3.8345e-02,  2.2153e-03, -2.5449e-02,\n",
      "         7.3994e-02, -8.9086e-02, -7.0133e-02,  2.3360e-02, -3.7967e-02,\n",
      "         3.6449e-01,  8.7289e-02, -1.3730e-02, -4.0335e-02,  9.1453e-03,\n",
      "         1.3115e-01, -3.1876e-02,  4.1707e-02,  6.6580e-03, -3.1702e-02,\n",
      "        -6.9562e-03, -1.3964e-02,  3.1259e-02,  5.2081e-02, -8.7700e-03,\n",
      "         6.2027e-02,  3.9866e-03,  3.7085e-02,  6.9625e-03, -4.5353e-02,\n",
      "        -4.5936e-02, -4.7751e-02, -1.2393e-02, -9.4233e-02, -4.0754e-02,\n",
      "         2.8093e-02,  4.6957e-02, -5.1367e-02,  4.1281e-02, -4.9334e-02,\n",
      "         5.9826e-03, -3.0414e-02,  6.5604e-03, -1.4088e-02, -2.9092e-02,\n",
      "         4.0886e-02, -4.6596e-02, -1.6262e-02,  1.1168e-02, -3.8112e-02,\n",
      "         7.8043e-02,  2.4254e-02, -4.0482e-02,  2.1042e-02, -2.7389e-02,\n",
      "         9.8253e-02, -7.7897e-02,  6.1152e-02, -1.2786e-01,  2.7121e-02,\n",
      "        -4.1936e-02,  2.2231e-02,  6.0525e-02,  4.9924e-02, -2.9452e-02,\n",
      "        -6.6132e-02,  1.7342e-02,  5.0480e-03,  7.9958e-02, -1.4857e-02,\n",
      "        -2.6854e-02,  6.0785e-02,  1.1634e-01, -1.8105e-02, -3.0417e-02,\n",
      "         2.7561e-02,  3.5151e-02, -5.0607e-02,  5.7016e-02, -1.1058e-02,\n",
      "         5.6519e-02, -1.8867e-02, -1.7996e-02,  3.1201e-02, -5.6396e-02,\n",
      "        -4.0019e-02,  5.7371e-02,  2.5591e-02, -5.2706e-02,  2.7222e-02,\n",
      "        -1.2649e-01,  5.8560e-03, -1.9015e-02, -2.2724e-03,  6.2894e-02,\n",
      "        -1.7388e-01,  2.8938e-02,  2.2116e-02,  1.5971e-02,  1.2000e-02,\n",
      "         6.0731e-02,  7.3260e-03,  1.3201e-01, -1.5783e-02,  4.6506e-02,\n",
      "        -2.7899e-02,  4.2483e-02, -2.9147e-02, -9.2941e-03,  4.5543e-02,\n",
      "        -6.4387e-03, -4.1477e-02,  3.8128e-03,  5.5919e-02, -3.2932e-02,\n",
      "         1.6099e-02, -6.4520e-02,  4.0178e-03, -2.5517e-02, -1.2830e-01,\n",
      "        -1.0194e-01, -1.1221e-02,  1.8321e-02, -2.4410e-02, -6.1464e-02,\n",
      "        -9.6066e-03, -5.7611e-02, -1.7421e-02, -1.4888e-02, -2.1559e-03,\n",
      "         4.4386e-02, -4.8366e-02,  3.4028e-02, -6.2879e-02, -2.3907e-02,\n",
      "         2.5231e-02, -3.1186e-02, -1.1364e-01,  4.0522e-02,  1.4350e-02,\n",
      "         4.1691e-02, -2.2301e-02, -7.0927e-02,  4.2017e-02,  6.1994e-02,\n",
      "         4.6840e-02,  5.5225e-02, -1.0839e-02,  4.0839e-02, -6.1900e-02,\n",
      "         2.6795e-02,  1.1623e-01,  2.1537e-02,  4.6706e-02,  4.3510e-02,\n",
      "        -5.5984e-02, -7.5878e-02, -1.4456e-01, -2.1498e-02,  3.5593e-02,\n",
      "        -5.0365e-02, -3.7307e-02, -1.4218e-02,  3.7678e-02,  2.6620e-02,\n",
      "        -3.9436e-02,  2.6094e-02, -1.2998e-01,  9.0322e-02,  4.4598e-02,\n",
      "         3.9002e-02, -4.7468e-04, -4.0988e-02,  4.0320e-02,  7.7261e-02,\n",
      "         2.1369e-02,  5.7156e-02,  4.0936e-02, -1.3352e-02, -1.7994e-02,\n",
      "         6.3618e-02,  5.9076e-02, -4.5136e-02,  2.5242e-02,  3.8890e-01,\n",
      "        -3.0602e-01,  3.8873e-02,  6.9421e-02,  1.1525e-02,  7.1667e-02,\n",
      "         2.0415e-02,  6.3292e-02,  3.9027e-02,  7.0006e-02,  6.6605e-02,\n",
      "        -1.0193e-02,  3.9742e-02, -3.0211e-02,  9.4335e-03,  1.8219e-02,\n",
      "         3.0888e-02, -5.2635e-02, -1.7593e-02, -8.1357e-03,  1.6580e-02,\n",
      "         5.2418e-02, -3.2394e-02, -1.0557e-02, -4.5694e-02, -3.6378e-03,\n",
      "        -4.5595e-03,  2.2049e-02, -1.3342e-02, -1.1562e-02, -1.9302e-02,\n",
      "         6.6502e-03,  2.0066e-02,  6.1327e-02,  1.9069e-02,  1.1715e-01,\n",
      "        -7.8155e-02, -1.0297e-02,  7.8561e-02,  6.9263e-03,  1.9901e-02,\n",
      "         3.8339e-02,  1.3989e-02, -8.0761e-03,  3.2563e-02,  2.7435e-02,\n",
      "        -5.7217e-03,  3.1232e-02,  2.0244e-02, -1.4396e-02,  1.1981e-01,\n",
      "         5.8153e-02,  5.1489e-02, -3.8983e-02,  1.2646e-03,  8.2045e-02,\n",
      "         8.9575e-03,  1.1725e-02,  3.9910e-03,  6.8989e-02,  1.8388e-02,\n",
      "         3.6768e-03, -9.2173e-02, -5.4111e-05, -4.8618e-02,  1.3042e-01,\n",
      "         5.9542e-02, -2.3551e-02, -1.3857e-01, -6.2895e-02, -2.9221e-02,\n",
      "         2.5589e-02,  4.4318e-02, -4.6107e-02,  1.9898e-02, -4.2811e-03,\n",
      "         1.5592e-02,  2.2303e-02, -6.4296e-02,  2.2451e-02, -1.3862e-02,\n",
      "         4.1372e-02,  3.9704e-02,  1.6719e-02, -6.1482e-02,  3.2137e-02,\n",
      "        -2.9562e-02,  1.4684e-02, -2.8574e-02, -3.8919e-02, -3.4208e-02,\n",
      "        -3.7310e-02,  4.5450e-02,  3.3517e-02,  5.8214e-02, -1.2275e-01,\n",
      "        -2.6694e-02,  3.7047e-02, -8.1020e-02, -3.5356e-02,  2.6342e-02,\n",
      "         1.7890e-02,  5.0927e-02, -2.5637e-02, -4.1648e-02, -1.5343e-03,\n",
      "         1.6817e-02, -2.1461e-02, -2.4146e-02, -5.5685e-02, -5.0002e-02,\n",
      "        -4.9231e-02,  9.9085e-03,  4.9858e-02,  1.3034e-02, -4.2224e-02,\n",
      "         1.6136e-02,  7.4182e-02,  1.3220e-02, -7.7645e-02, -1.4152e-02,\n",
      "         8.2275e-02,  2.2280e-02, -3.8874e-02, -6.4378e-01,  7.1428e-02,\n",
      "         8.7384e-02,  2.3851e-02,  4.6298e-02, -4.3605e-02, -3.6593e-02,\n",
      "         5.4288e-02,  3.6484e-02,  5.1660e-02, -4.6720e-02,  1.1488e-02,\n",
      "         1.0392e-03, -6.7187e-02,  6.4843e-02,  2.4149e-02,  3.2276e-03,\n",
      "         3.4351e-02, -4.6224e-02, -3.3864e-03, -2.3286e-02,  6.2609e-03,\n",
      "        -5.6694e-03,  9.3846e-03,  3.8671e-02,  1.5643e-02, -9.7819e-02,\n",
      "        -3.4346e-02,  1.0776e-01,  4.6475e-02, -2.1796e-02, -1.0123e-01,\n",
      "         9.5037e-03, -8.5888e-03,  2.7803e-02,  6.1494e-02, -3.1501e-02,\n",
      "        -7.5771e-04, -3.8617e-02,  3.3574e-03, -8.0889e-04,  2.2524e-01,\n",
      "         1.9737e-02,  1.7454e-01, -1.9330e-02, -6.7931e-03, -6.1890e-02,\n",
      "        -1.0091e-02, -3.8527e-02,  1.0535e-02, -2.6415e-03,  5.9733e-03,\n",
      "         6.9689e-03, -6.0660e-02, -9.5587e-03, -2.7253e-02,  4.3472e-02,\n",
      "        -1.1318e-02,  5.3104e-02,  1.6929e-01, -2.8873e-02,  1.2910e-02,\n",
      "         2.6885e-02,  2.3856e-02, -3.9623e-02, -4.9230e-02,  1.7523e-02,\n",
      "        -6.5107e-02,  1.8903e-02, -3.1567e-02, -6.4106e-02, -8.0717e-02,\n",
      "        -4.4473e-02,  2.1733e-02,  6.5894e-05,  7.4213e-02,  2.9008e-02,\n",
      "         4.1182e-02, -5.0136e-02,  6.8411e-02, -2.0343e-02,  2.3844e-02,\n",
      "         4.1738e-02,  5.7721e-02,  7.0955e-02, -1.0454e-01, -6.5782e-02,\n",
      "        -3.7556e-02,  3.5922e-03,  7.9681e-02, -1.9745e-02,  1.3927e-01,\n",
      "         1.9530e-02,  3.3480e-02, -3.7564e-02,  5.9669e-02,  8.3967e-02,\n",
      "        -1.5275e-02, -6.2722e-01, -8.2774e-02,  6.7602e-02, -3.3457e-02,\n",
      "         1.0244e-02,  1.6769e-02, -6.8549e-03, -1.9418e-02,  1.0233e-01,\n",
      "        -5.1207e-02,  3.1827e-02,  3.2509e-02,  6.0340e-02, -5.4452e-02,\n",
      "        -2.6766e-02,  3.7258e-02, -6.8405e-02, -6.8623e-02,  1.0060e-02,\n",
      "        -2.5261e-01, -1.4597e-02, -3.1373e-02,  7.6800e-02,  1.9468e-03,\n",
      "         5.3684e-02,  1.7839e-02, -1.9521e-02,  1.2063e-02,  5.6600e-02,\n",
      "         6.9129e-02,  2.9140e-02,  3.5718e-02, -6.8014e-02,  1.8970e-02,\n",
      "         4.1429e-03,  1.3008e-01,  7.3757e-02,  1.0626e+01, -3.0615e-02,\n",
      "         5.2813e-02,  7.5948e-03, -4.0732e-02, -9.5236e-02,  6.2624e-02,\n",
      "        -5.5481e-02,  3.7686e-02,  8.2724e-02,  2.5099e-02,  5.2817e-02,\n",
      "        -8.0583e-02,  1.3122e-03,  4.9456e-02,  1.8396e-02, -3.5808e-02,\n",
      "        -2.8766e-02,  3.1161e-02,  1.0395e-02,  2.0267e-02,  1.6590e-02,\n",
      "         2.7162e-02,  1.5458e-02, -7.0749e-02,  2.0193e-02,  2.4404e-02,\n",
      "        -7.3030e-03, -1.1997e-02,  5.5494e-02,  3.8639e-02,  6.8349e-03,\n",
      "         3.8789e-02,  8.2224e-02,  4.4766e-02, -2.2098e-02, -2.9327e-02,\n",
      "         1.0933e-01,  2.6871e-02,  7.8642e-02,  7.8752e-02, -8.4766e-03,\n",
      "        -2.1714e-02, -6.9581e-02,  7.0452e-02,  8.0374e-02, -7.8809e-02,\n",
      "         7.5958e-02,  2.1049e-02,  6.9318e-02,  5.7389e-02, -5.7874e-02,\n",
      "         7.7794e-02,  2.2832e-02, -1.1636e-03,  1.3306e-02,  5.0845e-02,\n",
      "        -4.6123e-02,  5.1342e-02,  5.6899e-03, -1.6863e-02,  1.0587e-01,\n",
      "        -3.3855e-02,  2.3333e-02, -7.8017e-02,  1.0907e-01,  5.1602e-02,\n",
      "         7.2694e-02, -5.6318e-02,  6.1420e-02, -1.6329e-02, -5.7924e-02,\n",
      "        -4.2250e-02, -4.4462e-02, -3.1569e-02, -3.4111e-02,  1.0526e-02,\n",
      "        -5.8715e-02,  6.2082e-02, -6.5881e-02,  6.4966e-02,  5.6511e-02,\n",
      "        -2.1077e-02, -7.5192e-02,  4.3386e-02,  4.2446e-02,  3.8136e-02,\n",
      "         1.2247e-02,  1.9135e-02,  1.3798e-02, -4.8367e-02,  2.6300e-02,\n",
      "         2.3802e-03, -4.4550e-02,  9.6329e-03,  2.0103e-02, -6.7250e-03,\n",
      "        -8.0066e-02,  5.7187e-02,  9.5645e-02, -2.6980e-02, -7.6931e-03,\n",
      "        -4.3313e-02,  9.8414e-02,  1.5808e-02,  3.9664e-02,  1.1557e-03,\n",
      "        -2.1402e-02,  1.5292e-02,  5.1760e-03, -4.5637e-02,  3.8667e-02,\n",
      "        -1.4027e-02,  1.2677e-02,  5.4594e-02,  1.9686e-02, -2.3445e-02,\n",
      "         2.3491e-02,  1.0803e-02,  5.8115e-02, -5.5127e-02,  2.6525e-02,\n",
      "         6.3507e-03, -5.8480e-03, -2.9382e-02, -3.2648e-02, -3.4448e-02,\n",
      "        -1.2084e-02,  5.1537e-03, -2.9366e-03, -2.2583e-02, -2.0299e-04,\n",
      "         4.0712e-02,  1.3005e-02,  1.3893e-02,  5.1988e-03, -1.3098e-02,\n",
      "         7.6709e-02,  4.0128e-02,  5.0076e-02, -3.9934e-02,  2.3553e-02,\n",
      "         4.1957e-03, -7.5957e-02, -5.1149e-02,  3.2416e-02,  8.4333e-02,\n",
      "         2.9083e-02,  4.2655e-02, -1.4674e-02,  4.8599e-02,  3.5525e-02,\n",
      "         4.9057e-02, -4.4807e-02, -2.4892e-02, -2.6372e-02, -2.2573e-02,\n",
      "         4.9481e-02, -2.5108e-02, -3.0319e-02,  6.3271e-02, -1.2386e-02,\n",
      "        -1.4924e-02,  4.8848e-02, -9.1122e-02, -3.0485e-02, -3.3263e-02,\n",
      "         3.4584e-02,  1.8720e-02, -1.2924e-02,  2.6743e-02,  2.2499e-02,\n",
      "        -4.8794e-02, -7.2000e-02,  1.2275e-02,  8.1876e-02,  1.1114e-01,\n",
      "        -8.8214e-02, -8.4798e-02,  1.4605e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForMaskedLM, RobertaModel,RobertaTokenizer\n",
    "device = \"cuda:0\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "bertmodel = RobertaModel.from_pretrained(\"roberta-base\", output_hidden_states=True).to(device)\n",
    "\n",
    "text = \"a  wadd  wda -1 13 2!! d\"\n",
    "input_ids = (tokenizer.encode(text,add_special_tokens=True, return_tensors='pt' )) #maybe get rid of square brackets\n",
    "with torch.no_grad():\n",
    "  outputs = bertmodel((input_ids.to(device)))\n",
    "  embeddings = outputs[0]\n",
    "  sentence_embedding = (embeddings[:,0,:])[0]\n",
    "\n",
    "  #print(outputs.shape)\n",
    "  print(embeddings.shape)\n",
    "  print(sentence_embedding.shape)\n",
    "  print(sentence_embedding)\n",
    "  # sentence_embedding = torch.mean(embeddings, dim=1)\n",
    "  # print(sentence_embedding)\n",
    "  # print(sentence_embedding.shape)\n",
    "\n",
    "\n",
    "\n",
    "def getBertEncodings(text):\n",
    "  input_ids = (tokenizer.encode(text,add_special_tokens=True, return_tensors='pt' )) #maybe get rid of square brackets\n",
    "  with torch.no_grad():\n",
    "    outputs = bertmodel((input_ids.to(device)))\n",
    "    embeddings = outputs[0]\n",
    "    sentence_embedding = torch.mean(embeddings, dim=1)\n",
    "    \n",
    "    \n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "# traindf['clean_title'] = traindf['clean_title'].apply(getBertEncodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what the fuck did you just fucking say about me you little bitch ill have you know i graduated top of my class in the navy seals and ive been involved in numerous secret raids on alquaeda and i have over confirmed kills i am trained in gorilla warfare and im the top sniper in the entire us armed forces you are nothing to me but just another target i will wipe you the fuck out with precision the likes of which has never been seen before on this earth mark my fucking words you think you can get away with saying that shit to me over the internet think again fucker as we speak i am contacting my secret network of spies across the usa and your ip is being traced right now so you better prepare for the storm maggot the storm that wipes out the pathetic little thing you call your life youre fucking dead kid i can be anywhere anytime and i can kill you in over seven hundred ways and thats just with my bare hands not only am i extensively trained in unarmed combat but i have access to the entire arsenal of the united states marine corps and i will use it to its full extent to wipe your miserable ass off the face of the continent you little shit if only you could have known what unholy retribution your little clever comment was about to bring down upon you maybe you would have held your fucking tongue but you couldnt you didnt and now youre paying the price you goddamn idiot i will shit fury all over you and you will drown in it youre fucking dead kiddowhat in davy jones locker did ye just bark at me ye scurvy bilgerat ill have ye know i be the meanest cutthroat on the seven seas and ive led numerous raids on fishing villages and raped over wenches i be trained in hitandrun pillaging and be the deadliest with a pistol of all the captains on the high seas ye be nothing to me but another source o swag ill have yer guts for garters and keel haul ye like never been done before hear me true you think ye can hide behind your newfangled computing device think twice on that scallywag as we parley i be contacting my secret network o pirates across the sea and yer port is being tracked right now so ye better prepare for the typhoon weevil the kind o monsoon thatll wipe ye off the map youre sharkbait fool i can sail anywhere in any waters and can kill ye in oer seven hundred ways and that be just with me hook and fist not only do i be top o the line with a cutlass but i have an entire pirate fleet at my beck and call and ill damned sure use it all to wipe yer arse off o the world ye dog if only ye had had the foresight to know what devilish wrath your jibe was about to incur ye might have belayed the comment but ye couldnt ye didnt and now yell pay the ultimate toll you buffoon ill shit fury all over ye and yell drown in the depths o it youre fish food now\n",
      "The expanded size of the tensor (613) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 613].  Tensor sizes: [1, 514]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "torch.set_printoptions(threshold=1090)\n",
    "f = open(\"train10bd.tsv\", 'x')\n",
    "c = csv.writer(f, delimiter='\\t')\n",
    "c.writerow([\"embedding\", \"label\"])\n",
    "with torch.no_grad():\n",
    "    for index, row in traindf.iterrows():\n",
    "        \n",
    "            try:\n",
    "                text = row['title']\n",
    "                        \n",
    "                    \n",
    "                embedding = getBertEncodings(text).to(device)\n",
    "                \n",
    "                row['title'] = embedding\n",
    "                c.writerow([row['title'], row['label']])\n",
    "            except Exception as e:\n",
    "                print(row['title'])\n",
    "                print(str(e))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test2bd.tsv\", 'x')\n",
    "c = csv.writer(f, delimiter='\\t')\n",
    "c.writerow([\"embedding\", \"label\"])\n",
    "with torch.no_grad():\n",
    "    for index, row in testdf.iterrows():\n",
    "        \n",
    "            try:\n",
    "                text = row['title']\n",
    "                        \n",
    "                    \n",
    "                embedding = getBertEncodings(text).to(device)\n",
    "                \n",
    "                row['title'] = embedding\n",
    "                c.writerow([row['title'], row['label']])\n",
    "            except:\n",
    "                print(row['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29a1f184ef47e50188ad724faf855c2d61f1ecd260e34595dbeba947cc1e9be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
