{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy        as np\n",
    "import pandas       as pd\n",
    "import tensorflow   as tf\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import operator\n",
    "import sklearn \n",
    "import re\n",
    "\n",
    "from mlxtend.plotting                           import plot_confusion_matrix\n",
    "from gensim.models                              import KeyedVectors\n",
    "from tqdm                                       import tqdm\n",
    "from sklearn                                    import metrics \n",
    "from sklearn.metrics                            import confusion_matrix\n",
    "from sklearn.model_selection                    import train_test_split\n",
    "\n",
    "from tensorflow                                 import keras\n",
    "from tensorflow.keras                           import layers\n",
    "from tensorflow.keras.layers                    import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D,GRU,Bidirectional, GlobalMaxPool1D \n",
    "from tensorflow.keras.models                    import Model,load_model, Sequential\n",
    "from tensorflow.keras.preprocessing.sequence    import pad_sequences\n",
    "from tensorflow.keras.utils                     import plot_model\n",
    "from tensorflow.keras.callbacks                 import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.optimizers                import Adam\n",
    "from tensorboard.plugins                        import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(doc):\n",
    "    corpus=[]\n",
    "    for text in tqdm(doc):\n",
    "        text=\" \".join([contraction_fix(w) for w in text.split()])\n",
    "        text=re.sub(r'[^a-z0-9A-Z]',\" \",text)\n",
    "        text=re.sub(r'[0-9]{1}',\"#\",text)\n",
    "        text=re.sub(r'[0-9]{2}','##',text)\n",
    "        text=re.sub(r'[0-9]{3}','###',text)\n",
    "        text=re.sub(r'[0-9]{4}','####',text)\n",
    "        text=re.sub(r'[0-9]{5,}','#####',text)\n",
    "        corpus.append(text)\n",
    "    return corpus\n",
    "\n",
    "def contraction_fix(word):\n",
    "    try:\n",
    "        a=contractions[word]\n",
    "    except KeyError:\n",
    "        a=word\n",
    "    return a\n",
    "\n",
    "def get_vocab(corpus):\n",
    "    vocab={}\n",
    "    for text in tqdm(corpus):\n",
    "        for word in text.split():\n",
    "            try:\n",
    "                vocab[word]+=1\n",
    "            except KeyError:\n",
    "                vocab[word]=1\n",
    "    vocab=dict(sorted(vocab.items(),reverse=True ,key=lambda item: item[1]))\n",
    "    return vocab\n",
    "\n",
    "def get_word_index(vocab):\n",
    "    word_index=dict((w,i+1) for i,w in enumerate(vocab.keys()))\n",
    "    return word_index\n",
    "\n",
    "def fit_one_hot(word_index,corpus):\n",
    "    sent=[]\n",
    "    for text in tqdm(corpus):\n",
    "        li=[]\n",
    "        for word in text.split():\n",
    "            try:\n",
    "                li.append(word_index[word])\n",
    "            except KeyError:\n",
    "                li.append(0)\n",
    "        sent.append(li)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_lstm(all_data):\n",
    "    MAX_FEATURES = 3000\n",
    "    MAX_LENGTH = 40 \n",
    "    EMBEDDING_SIZE = 128\n",
    "    HIDDEN_LAYER_SIZE = 64\n",
    "    BATCH_SIZE = 512\n",
    "    NUM_EPOCHS = 5\n",
    "    FEATURE_VEC=300\n",
    "    VERBOSE = False\n",
    "\n",
    "    train, ds_set = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "    test, validation = train_test_split(ds_set, test_size=0.3, random_state=42)\n",
    "\n",
    "    contractions = {\"I'm\": 'I am', \"I'm'a\": 'I am about to', \"I'm'o\": 'I am going to', \"I've\": 'I have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'd\": 'I would', \"I'd've\": 'I would have', 'Whatcha': 'What are you', \"amn't\": 'am not', \"ain't\": 'are not', \"aren't\": 'are not', \"'cause\": 'because', \"can't\": 'can not', \"can't've\": 'can not have', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"daren't\": 'dare not', \"daresn't\": 'dare not', \"dasn't\": 'dare not', \"didn't\": 'did not', 'didn’t': 'did not', \"don't\": 'do not', 'don’t': 'do not', \"doesn't\": 'does not', \"e'er\": 'ever', \"everyone's\": 'everyone is', 'finna': 'fixing to', 'gimme': 'give me', \"gon't\": 'go not', 'gonna': 'going to', 'gotta': 'got to', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he've\": 'he have', \"he's\": 'he is', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he'd\": 'he would', \"he'd've\": 'he would have', \"here's\": 'here is', \"how're\": 'how are', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how's\": 'how is', \"how'll\": 'how will', \"isn't\": 'is not', \"it's\": 'it is', \"'tis\": 'it is', \"'twas\": 'it was', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it'd\": 'it would', \"it'd've\": 'it would have', 'kinda': 'kind of', \"let's\": 'let us', 'luv': 'love', \"ma'am\": 'madam', \"may've\": 'may have', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"ne'er\": 'never', \"o'\": 'of', \"o'clock\": 'of the clock', \"ol'\": 'old', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"o'er\": 'over', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shalln't\": 'shall not', \"shan't've\": 'shall not have', \"she's\": 'she is', \"she'll\": 'she will', \"she'd\": 'she would', \"she'd've\": 'she would have', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"somebody's\": 'somebody is', \"someone's\": 'someone is', \"something's\": 'something is', 'sux': 'sucks', \"that're\": 'that are', \"that's\": 'that is', \"that'll\": 'that will', \"that'd\": 'that would', \"that'd've\": 'that would have', 'em': 'them', \"there're\": 'there are', \"there's\": 'there is', \"there'll\": 'there will', \"there'd\": 'there would', \"there'd've\": 'there would have', \"these're\": 'these are', \"they're\": 'they are', \"they've\": 'they have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they'd\": 'they would', \"they'd've\": 'they would have', \"this's\": 'this is', \"those're\": 'those are', \"to've\": 'to have', 'wanna': 'want to', \"wasn't\": 'was not', \"we're\": 'we are', \"we've\": 'we have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we'd\": 'we would', \"we'd've\": 'we would have', \"weren't\": 'were not', \"what're\": 'what are', \"what'd\": 'what did', \"what've\": 'what have', \"what's\": 'what is', \"what'll\": 'what will', \"what'll've\": 'what will have', \"when've\": 'when have', \"when's\": 'when is', \"where're\": 'where are', \"where'd\": 'where did', \"where've\": 'where have', \"where's\": 'where is', \"which's\": 'which is', \"who're\": 'who are', \"who've\": 'who have', \"who's\": 'who is', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who'd\": 'who would', \"who'd've\": 'who would have', \"why're\": 'why are', \"why'd\": 'why did', \"why've\": 'why have', \"why's\": 'why is', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"you're\": 'you are', \"you've\": 'you have', \"you'll've\": 'you shall have', \"you'll\": 'you will', \"you'd\": 'you would', \"you'd've\": 'you would have', 'jan.': 'january', 'feb.': 'february', 'mar.': 'march', 'apr.': 'april', 'jun.': 'june', 'jul.': 'july', 'aug.': 'august', 'sep.': 'september', 'oct.': 'october', 'nov.': 'november', 'dec.': 'december', 'I’m': 'I am', 'I’m’a': 'I am about to', 'I’m’o': 'I am going to', 'I’ve': 'I have', 'I’ll': 'I will', 'I’ll’ve': 'I will have', 'I’d': 'I would', 'I’d’ve': 'I would have', 'amn’t': 'am not', 'ain’t': 'are not', 'aren’t': 'are not', '’cause': 'because', 'can’t': 'can not', 'can’t’ve': 'can not have', 'could’ve': 'could have', 'couldn’t': 'could not', 'couldn’t’ve': 'could not have', 'daren’t': 'dare not', 'daresn’t': 'dare not', 'dasn’t': 'dare not', 'doesn’t': 'does not', 'e’er': 'ever', 'everyone’s': 'everyone is', 'gon’t': 'go not', 'hadn’t': 'had not', 'hadn’t’ve': 'had not have', 'hasn’t': 'has not', 'haven’t': 'have not', 'he’ve': 'he have', 'he’s': 'he is', 'he’ll': 'he will', 'he’ll’ve': 'he will have', 'he’d': 'he would', 'he’d’ve': 'he would have', 'here’s': 'here is', 'how’re': 'how are', 'how’d': 'how did', 'how’d’y': 'how do you', 'how’s': 'how is', 'how’ll': 'how will', 'isn’t': 'is not', 'it’s': 'it is', '’tis': 'it is', '’twas': 'it was', 'it’ll': 'it will', 'it’ll’ve': 'it will have', 'it’d': 'it would', 'it’d’ve': 'it would have', 'let’s': 'let us', 'ma’am': 'madam', 'may’ve': 'may have', 'mayn’t': 'may not', 'might’ve': 'might have', 'mightn’t': 'might not', 'mightn’t’ve': 'might not have', 'must’ve': 'must have', 'mustn’t': 'must not', 'mustn’t’ve': 'must not have', 'needn’t': 'need not', 'needn’t’ve': 'need not have', 'ne’er': 'never', 'o’': 'of', 'o’clock': 'of the clock', 'ol’': 'old', 'oughtn’t': 'ought not', 'oughtn’t’ve': 'ought not have', 'o’er': 'over', 'shan’t': 'shall not', 'sha’n’t': 'shall not', 'shalln’t': 'shall not', 'shan’t’ve': 'shall not have', 'she’s': 'she is', 'she’ll': 'she will', 'she’d': 'she would', 'she’d’ve': 'she would have', 'should’ve': 'should have', 'shouldn’t': 'should not', 'shouldn’t’ve': 'should not have', 'so’ve': 'so have', 'so’s': 'so is', 'somebody’s': 'somebody is', 'someone’s': 'someone is', 'something’s': 'something is', 'that’re': 'that are', 'that’s': 'that is', 'that’ll': 'that will', 'that’d': 'that would', 'that’d’ve': 'that would have', 'there’re': 'there are', 'there’s': 'there is', 'there’ll': 'there will', 'there’d': 'there would', 'there’d’ve': 'there would have', 'these’re': 'these are', 'they’re': 'they are', 'they’ve': 'they have', 'they’ll': 'they will', 'they’ll’ve': 'they will have', 'they’d': 'they would', 'they’d’ve': 'they would have', 'this’s': 'this is', 'those’re': 'those are', 'to’ve': 'to have', 'wasn’t': 'was not', 'we’re': 'we are', 'we’ve': 'we have', 'we’ll': 'we will', 'we’ll’ve': 'we will have', 'we’d': 'we would', 'we’d’ve': 'we would have', 'weren’t': 'were not', 'what’re': 'what are', 'what’d': 'what did', 'what’ve': 'what have', 'what’s': 'what is', 'what’ll': 'what will', 'what’ll’ve': 'what will have', 'when’ve': 'when have', 'when’s': 'when is', 'where’re': 'where are', 'where’d': 'where did', 'where’ve': 'where have', 'where’s': 'where is', 'which’s': 'which is', 'who’re': 'who are', 'who’ve': 'who have', 'who’s': 'who is', 'who’ll': 'who will', 'who’ll’ve': 'who will have', 'who’d': 'who would', 'who’d’ve': 'who would have', 'why’re': 'why are', 'why’d': 'why did', 'why’ve': 'why have', 'why’s': 'why is', 'will’ve': 'will have', 'won’t': 'will not', 'won’t’ve': 'will not have', 'would’ve': 'would have', 'wouldn’t': 'would not', 'wouldn’t’ve': 'would not have', 'y’all': 'you all', 'y’all’re': 'you all are', 'y’all’ve': 'you all have', 'y’all’d': 'you all would', 'y’all’d’ve': 'you all would have', 'you’re': 'you are', 'you’ve': 'you have', 'you’ll’ve': 'you shall have', 'you’ll': 'you will', 'you’d': 'you would', 'you’d’ve': 'you would have'}\n",
    "\n",
    "    vocab = get_vocab(train.text)\n",
    "    top_feat = dict(list(vocab.items())[:MAX_FEATURES])\n",
    "    word_index = get_word_index(top_feat)\n",
    "    len(word_index.keys())\n",
    "\n",
    "    encoded_docs    = fit_one_hot(word_index,train.text)\n",
    "    padded_doc      = pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding=\"post\")\n",
    "\n",
    "    train_text      = Preprocess(train.text)\n",
    "    test_text       = Preprocess(test.text)\n",
    "    validation_text = Preprocess(validation.text)\n",
    "\n",
    "    vocab       = get_vocab(train_text)\n",
    "    top_feat    = dict(list(vocab.items())[:MAX_FEATURES])\n",
    "    word_index  = get_word_index(top_feat)\n",
    "\n",
    "    encoded_docs = fit_one_hot(word_index,train_text)\n",
    "    train_padded = pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding=\"post\")\n",
    "\n",
    "    encoded_docs = fit_one_hot(word_index,test_text)\n",
    "    test_padded  = pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding=\"post\")\n",
    "\n",
    "    encoded_docs        = fit_one_hot(word_index,validation_text)\n",
    "    validation_padded   = pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding=\"post\")\n",
    "\n",
    "    inp = Input(shape=(MAX_LENGTH,))\n",
    "    x = Embedding(MAX_FEATURES + 1, FEATURE_VEC)(inp)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    x = Conv1D(64,5,activation=\"relu\")(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    print(model.summary())\n",
    "    ### defining some callbacks\n",
    "    opt=Adam(learning_rate=0.002)\n",
    "    bin_loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.2, name='binary_crossentropy')\n",
    "\n",
    "    ## defining the call backs\n",
    "\n",
    "    #see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "    early_stopping=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=3,mode=\"min\",restore_best_weights=True)\n",
    "\n",
    "    ### Now reducing the learning rate when the model is not improvinig \n",
    "    reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",factor=0.2,patience=2, verbose=1,  mode=\"auto\")\n",
    "\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    my_callbacks=[early_stopping,reduce_lr, tensorboard_callback]\n",
    "    model.compile(loss=bin_loss, optimizer=opt, metrics=['accuracy'])\n",
    "    #with GPU/CPU\n",
    "        \n",
    "    history = model.fit(train_padded, train.target, \n",
    "                            batch_size=BATCH_SIZE,  #512\n",
    "                            epochs=NUM_EPOCHS,\n",
    "                            validation_data=(test_padded, test.target),\n",
    "                            callbacks=my_callbacks)\n",
    "    len(history.history['val_loss'])\n",
    "\n",
    "    # Plot history: MAE\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history.history['loss'], label='MAE (training data)')\n",
    "    plt.plot(history.history['val_loss'], label='MAE (test data)')\n",
    "    plt.title('MAE for Clickbait')\n",
    "    plt.ylabel('MAE value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    score = model.evaluate(test_padded, test.target, verbose=1)\n",
    "    print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "\n",
    "    #Check the model against the holdout set\n",
    "    validation_results = model.predict(validation_padded)\n",
    "    target = validation.target.array\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    total = len(validation)\n",
    "\n",
    "    for i in range(total):\n",
    "        guessed = int(np.round((validation_results[i]),0))\n",
    "        if target[i] == guessed:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            if VERBOSE:\n",
    "                print(f\"Text: {validation_text[i]}\\n\\tGuessed Wrong: {guessed}({validation_results[i]}), Actual: {target[i]}\")\n",
    "\n",
    "    print(f'Right: {right}, Wrong: {wrong}, Accuracy {right*100/total:2.2f}%')\n",
    "\n",
    "    #%time\n",
    "    s1 = pd.merge(  train, validation, how='inner', on=['text'])\n",
    "    s2 = pd.merge( validation, test, how='inner', on=['text'])\n",
    "    print(f'Interractions between train and validation: {s1.size}')\n",
    "    print(f'Interractions between test and validation: {s1.size}')\n",
    "\n",
    "    preds = [round(i[0]) for i in model.predict(test_padded)]\n",
    "    cm = confusion_matrix(test.target, preds)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)\n",
    "    plt.xticks(range(2), ['Not clickbait', 'Clickbait'], fontsize=16)\n",
    "    plt.yticks(range(2), ['Not clickbait', 'Clickbait'], fontsize=16)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
